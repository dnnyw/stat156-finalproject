{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7a056f20-5e8b-4cda-93a2-cdd28cae59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import ast\n",
    "\n",
    "# Source code downloaded\n",
    "from vincenty import vincenty_inverse\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72374b77-d647-4a1f-906d-b7c05ad9b548",
   "metadata": {},
   "source": [
    "# Cleaning EPA Data - Creating the condensed .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6c88b49b-7f0b-420c-a48b-ad2e6dee4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "NOAA_dir = current_dir + r'\\NOAA Weather Data'\n",
    "EPA_dir = current_dir + r'\\EPA Ozone Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "19183764-1178-4513-b814-b70392134424",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = os.listdir(EPA_dir + r'\\Raw EPA Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "91c053d2-8cf1-41a8-b095-cf545719b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ozone_id(statelist, countylist, sitelist):\n",
    "    \"\"\"\n",
    "    Helper function for creating ozoneID's based on an EPA dataset\n",
    "    \n",
    "    returns: 3 lists appended together into tuples to be added into a column\n",
    "    \"\"\"\n",
    "    return [(a,b,c) for a, b, c in zip(statelist, countylist, sitelist)]\n",
    "\n",
    "def append_ozone_id(ozone_df):\n",
    "    \"\"\"\n",
    "    Creates a copy of the dataframe and adds a new column that concatenates the state code, county code, and site number\n",
    "    into a tuple to make an individual identifier for each ozone reporting location in that year. \n",
    "    \n",
    "    returns: dataframe with ozoneID\n",
    "    \"\"\"\n",
    "    temp = ozone_df.copy() \n",
    "    temp[\"ozoneID\"] = create_ozone_id(temp[\"State Code\"], temp[\"County Code\"], temp[\"Site Num\"])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ea2409df-2033-41cb-9cdd-0e9f6bcaf9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_period = ['09:00', '10:00', '11:00', '12:00', '13:00', '14:00', \n",
    "                      '15:00', '16:00', '17:00', '18:00', '19:00', '20:00']\n",
    "\n",
    "def monitor_day_filter(grouped_dataframe):\n",
    "    \"\"\"\n",
    "    Used in split-apply-combine after grouping by monitor-day in order to filter out days that have \n",
    "    less than 9 observations in the hours from 9am to 9pm\n",
    "    \"\"\"\n",
    "    times = grouped_dataframe[\"Time Local\"]\n",
    "    indicator = [time in time_period for time in times]\n",
    "    return sum(indicator) >= 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6d898-7846-4301-a07a-3f44003ad5a6",
   "metadata": {},
   "source": [
    "## Construct 2 measures of ozone concentrations at the monitor-day level\n",
    "\n",
    "- Daily Maximum: Groupby date, then return the maximum of that day\n",
    "\n",
    "- Daily 8-hour Maximum: Groupby date, then average hours 0-8, 8-4, 4-12, return the maximum of that \n",
    "\n",
    "- Disqualify all monitor-days for which observations are not recorded for at least 9 hours between 9AM and 9PM, disqualify all monitors that have less than 75% of the days recorded from June1 to August 31,\n",
    "\n",
    "- Disqualify monitors that are in counties close to other counties that have more stringent regulation (?????) \n",
    "\n",
    "Do this in one group by, write an apply function to return both of these as a new data frame per date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1c05e8b1-d5f6-40bd-9702-71d0f98fae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_time(time_str):\n",
    "    time_int = int(time_str[0:2])\n",
    "    if time_int < 8:\n",
    "        return 1\n",
    "    elif 8 <= time_int < 16:\n",
    "        return 2\n",
    "    elif  16 <= time_int:\n",
    "        return 3\n",
    "    else:\n",
    "        raise ValueError('Time Local date was invalid?') \n",
    "\n",
    "def calculate_max_8hrmax(grouped_dataframe):\n",
    "    \"\"\"\n",
    "    Used in split-apply-combine after grouping by monitor-day - calculate the maximum of the day as well as the \n",
    "    8 hour maximum - 8 hour maximum \n",
    "    \"\"\"\n",
    "       \n",
    "    grouped_dataframe[\"Time Chunk\"] = grouped_dataframe[\"Time Local\"].apply(classify_time)\n",
    "    \n",
    "    samples_all = grouped_dataframe[\"Sample Measurement\"]\n",
    "    \n",
    "    mean_1 = np.mean(grouped_dataframe.loc[grouped_dataframe[\"Time Chunk\"] == 1, \"Sample Measurement\"])\n",
    "    mean_2 = np.mean(grouped_dataframe.loc[grouped_dataframe[\"Time Chunk\"] == 2, \"Sample Measurement\"])\n",
    "    mean_3 = np.mean(grouped_dataframe.loc[grouped_dataframe[\"Time Chunk\"] == 3, \"Sample Measurement\"])\n",
    "    \n",
    "    # CHANGE THIS TO MAX FUCK\n",
    "    daily_max = np.max(samples_all)\n",
    "    eight_hour_max = max(mean_1, mean_2, mean_3)\n",
    "    \n",
    "    output = grouped_dataframe.iloc[[0],:]\n",
    "    \n",
    "    output[\"Daily Max Ozone\"] = daily_max\n",
    "    output[\"Daily 8hr Mean Ozone\"] = eight_hour_max\n",
    "    return output\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "666ed6eb-3cbc-42c7-9272-6f4b529d5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def is_summer(date_str):\n",
    "    \"\"\"\n",
    "    Checks if a dat str (in the format of the tablse) is in the summer \n",
    "    \"\"\"\n",
    "    month = int(re.search(r'\\d{4}-(\\d{2})-\\d{2}', date_str).group(1))\n",
    "    return 6 <= month <= 8\n",
    "\n",
    "\n",
    "def monitor_year_filter(grouped_ozone_id):\n",
    "    \"\"\" \n",
    "    Used in split-apply-combine after grouping by monitor in order to filter out entire monitors that do not have \n",
    "    observations in 25% or more during the summer ozone months Do this after calculating the maximum and 8hr maximums \n",
    "    \n",
    "    DO THIS BEFORE APPLYING MAX DATE BECAUSE MAX DATE SLOW AS HELL \n",
    "    \"\"\"\n",
    "    # 92 days between June 1 and August 31, need 75% observations or more == only accept if greater than or equal to 69\n",
    "    dates = grouped_ozone_id[\"Date Local\"].unique()\n",
    "    return sum(is_summer(dates)) >= 69\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43527ecc-440d-4c12-8a42-9aca18a4d345",
   "metadata": {},
   "source": [
    "## For Loop Instructions \n",
    "- Read the csv, then append ozone_id to get the ozone_Ids\n",
    "- First group by date and ozone_id and filter out monitor-dates using monitor-day-filter \n",
    "    - This will net us with a dataframe that only has monitor dates for enough observations in the 9 hours  \n",
    "- Apply the calculate max-8hrmax function to grouped monitor-days \n",
    "    - This will result with each day ending up justb being a single observation with 2 new columns. \n",
    "- Now group by monitor and look at whether or not this year had enough values, return final data frame after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6edcb7-3590-4a1e-b291-6cd81f743a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 1989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danny\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (17,23) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv done!\n",
      "finished appending for hourly_44201_1989.zip\n",
      "finished dayfilter for hourly_44201_1989.zip\n",
      "finished yearfilter for hourly_44201_1989.zip\n"
     ]
    }
   ],
   "source": [
    "zip_files = os.listdir(EPA_dir + r'\\Raw EPA Data')\n",
    "zip_files\n",
    "\n",
    "for filename in zip_files: \n",
    "    \n",
    "    year = filename[13:17]\n",
    "    \n",
    "    if False:\n",
    "#     if f\"filtered_{year}_EPA.csv\" in os.listdir(EPA_dir + r'\\Filtered EPA Data'):\n",
    "        print(f\"Skipping {year}. already done\")\n",
    "        continue\n",
    "    else: \n",
    "        print(f\"working on {year}\")\n",
    "        filepath = EPA_dir + r\"\\\\Raw EPA Data\\\\\" + filename \n",
    "        temp = pd.read_csv(filepath)\n",
    "        print(\"read csv done!\")\n",
    "        \n",
    "        temp = append_ozone_id(temp)\n",
    "        print(f\"finished appending for {filename}\")\n",
    "        \n",
    "        temp = temp.groupby([\"ozoneID\", \"Date Local\"]).filter(monitor_day_filter)\n",
    "        print(f\"finished dayfilter for {filename}\")\n",
    "        \n",
    "        temp = temp.groupby(\"ozoneID\").filter(monitor_year_filter)\n",
    "        print(f\"finished yearfilter for {filename}\")\n",
    "        \n",
    "        temp = temp.groupby([\"ozoneID\", \"Date Local\"], group_keys=False).apply(calculate_max_8hrmax)\n",
    "        print(f\"finished {filename}, exporting to csv!\")\n",
    "\n",
    "        path = EPA_dir + r'\\Filtered EPA Data\\filtered_' + year + \"_EPA.csv\"\n",
    "        temp.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765b865-c860-4a73-9968-0988ae7e8868",
   "metadata": {},
   "source": [
    "Note: (13, 85, 1) has 48 counts for some reason - it's just duplicated values can ignore because not gonna affect mean and 8hr mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc820f23-7925-40d4-a954-0fa43a4af433",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating the lists of state_code county_code pairs that need to be deleted  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08986b9-e23a-49bd-a7a3-fad0b1276c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_list = pd.read_stata(current_dir + \"\\Author Data\\AER20090377_CountyList.dta\")\n",
    "county_list[\"fips\"] = county_list[\"fips\"].astype(int)\n",
    "county_list = county_list.rename(columns = {\"state_code\": \"State Code\", \"county_code\":\"County Code\"})\n",
    "county_list = county_list.drop(columns = \"county_desc\")\n",
    "county_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74060fc7-879c-4e58-9584-ec1c393d7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_list = pd.read_stata(current_dir + \"\\Author Data\\AER20090377_NeighborData.dta\")\n",
    "neighbor_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdd586-b3e9-417c-896c-466cd94bb3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If deleting too many values, then make this so that it only counts the neighbors that are == 1\n",
    "neighbor_fips = neighbor_list[\"fips\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e0af7-6ca6-4653-90eb-13280a538909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urban_df(year):\n",
    "    \"\"\"\n",
    "    Returns the dataframe of the urban designations as extracted from \"Getting Urban Designation.ipynb\"|\n",
    "    \"\"\"\n",
    "    assert 1989 <= year <= 2003, \"Bad year input\"\n",
    "    df = pd.read_csv(current_dir + r\"\\Author Data\\county_urban_designation\\county_urban_designation_\" + str(year) + \".csv\")\n",
    "    df[\"urban\"] = [int(text[1]) for text in df[\"urban\"]]\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c7148-a49b-4729-a499-fb670fc25dd6",
   "metadata": {},
   "source": [
    "### Filtering out the above counties for each file, selecting only the summer months, and then also storing necessary data to create the summary table, also adding rural/urban/suburban\n",
    "\n",
    "- First add all of county_list data to the normal data\n",
    "- Filter based on that \n",
    "- Apply county filter from above\n",
    "- Store the number of remaining ozone_locations\n",
    "- Store the number of monitor-days left\n",
    "- Count how many are rural/urban "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793fac1-6005-4487-ad83-3f93e2d15e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83f17b-03f4-4b6c-b339-3f31582b63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dir_list = os.listdir(EPA_dir + r'\\Filtered EPA Data')\n",
    "filtered_files = [file for file in filtered_dir_list if file.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7f1b7-ba2d-4fa3-a2b2-458e8a08402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summer_output_folder_path = EPA_dir + r'\\Filtered EPA Data\\onlysummer\\\\' \n",
    "second_level_cleaning_folder_path = EPA_dir + r'\\Filtered EPA Data\\second_cleaning\\\\' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06d63f-74aa-4198-8cdc-935873dfeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name in filtered_files:\n",
    "    \n",
    "    year = int(df_name[9:13])\n",
    "    \n",
    "    if False:\n",
    "#     if r\"summer_only_EPA_\" + str(year) + \".csv\" in os.listdir(summer_output_folder_path):\n",
    "        print(r\"summer_only_EPA_\" + str(year) + \".csv already exists, skipping\")\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        year = int(df_name[9:13])\n",
    "        \n",
    "        print(\"working on \" + str(year))\n",
    "        temp = pd.read_csv(EPA_dir + r'\\Filtered EPA Data\\\\' + df_name, low_memory = False)\n",
    "\n",
    "        \n",
    "        # IF THERE IS CANADA, DELETE THOSE ROWS LMAO \n",
    "        \n",
    "        if \"CC\" in temp[\"State Code\"].unique(): \n",
    "            temp = temp[temp[\"State Code\"] != \"CC\"]\n",
    "            temp[\"State Code\"] = temp[\"State Code\"].astype(int)\n",
    "            \n",
    "        \n",
    "        # add information from county_list file -- add fips \n",
    "        temp = temp.merge(county_list.iloc[:,0:3], on = [\"State Code\", \"County Code\"]) # now we have a bunch of extra info but that's okay \n",
    "\n",
    "        # remove counties that are in the neighbors list \n",
    "        neighbor_indicator = [county_fip not in neighbor_fips for county_fip in temp[\"fips\"]]\n",
    "        temp = temp[neighbor_indicator] \n",
    "\n",
    "\n",
    "        # add urban designation - for missing observations with NaN, fill with zero.\n",
    "        # 3 = suburban, 2 = rural, 1 = urban\n",
    "        temp = temp.merge(get_urban_df(year), on = \"ozoneID\", how = \"left\").fillna({\"urban\":0})     \n",
    "        \n",
    "        temp = temp.drop(columns = [\"Parameter Code\", \"POC\", \"Datum\", \"Parameter Name\", \n",
    "                                    \"Time Local\", \"Date GMT\", \"Time GMT\", \"Units of Measure\", \n",
    "                                    \"Uncertainty\", \"Qualifier\", \"Method Type\", \"Method Code\", \n",
    "                                    \"Method Name\", \"MDL\", \"Date of Last Change\", \"Time Chunk\"])\n",
    "\n",
    "        temp.to_csv(second_level_cleaning_folder_path + r\"condensed_EPA_\" + str(year) + \".csv\")\n",
    "\n",
    "#         # select only observations that are in the months of June, July, August \n",
    "#         summer_indicator = [int(date[5:7]) in [6, 7, 8] for date in temp[\"Date Local\"]]\n",
    "#         summer_only = temp[summer_indicator]\n",
    "\n",
    "#         summer_only.to_csv(summer_output_folder_path + r\"summer_only_EPA_\" + str(year) + \".csv\")\n",
    "\n",
    "        #get statistics\n",
    "\n",
    "#         num_obs = summer_only.shape[0]\n",
    "#         num_counties = len(summer_only[\"fips\"].unique())\n",
    "#         num_monitors = len(summer_only[\"ozoneID\"].unique())\n",
    "#         urban_0_count = sum(summer_only.groupby(\"ozoneID\")[\"urban\"].unique() == 0)\n",
    "#         urban_1_count = sum(summer_only.groupby(\"ozoneID\")[\"urban\"].unique() == 1)\n",
    "#         urban_2_count = sum(summer_only.groupby(\"ozoneID\")[\"urban\"].unique() == 2)\n",
    "#         urban_3_count = sum(summer_only.groupby(\"ozoneID\")[\"urban\"].unique() == 3)\n",
    "\n",
    "#         statistics[year] = {\"Observations\":num_obs,\n",
    "#                             \"Counties\":num_counties,\n",
    "#                             \"Total Monitors\":num_monitors,\n",
    "#                             \"Urban_1\":urban_1_count,\n",
    "#                             \"Urban_2\":urban_2_count,\n",
    "#                             \"Urban_3\":urban_3_count,\n",
    "#                             \"Urban_0\":urban_0_count,\n",
    "#                            }\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aab260-135a-41de-8ae8-060ea696ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_table = pd.DataFrame(statistics).T\n",
    "# summary_table.to_csv(\"summary_table2.csv\")\n",
    "# summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f9706e-62b9-4a8e-8eee-55a451809361",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"summary_table2.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92afa257-6ad0-44e9-b86c-fef832facfca",
   "metadata": {},
   "source": [
    "# Cleaning NOAA Data + Joining with EPA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc3761-0d48-4e11-aee0-f0c2143cb0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "NOAA_dir = current_dir + r'\\NOAA Weather Data'\n",
    "EPA_dir = current_dir + r'\\EPA Ozone Data'\n",
    "\n",
    "filtered_EPA_dir = current_dir + r'/EPA Ozone Data/Filtered EPA Data/second_cleaning/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e19dd-494f-4236-9818-707e2c74590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_EPA_files = [file for file in os.listdir(filtered_EPA_dir) if file.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdafa6e-31a9-4a51-a267-10c02d45e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA_files = [file for file in os.listdir(NOAA_dir) if file.endswith(\".csv.gz\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6303264-0cb9-452e-8b11-1e5439e640d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_station_codes = pd.read_csv(NOAA_dir + r\"\\us_station_codes.csv\", index_col = 0).drop(columns = [\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eecc9e-ade2-4f72-b430-071f9829feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ozoneID_coords(ozone_df):\n",
    "    \"\"\"\n",
    "    Groups entire dataframe by ozoneID and then applies lambda function that extracts the first entry of Latitude, Longitude\n",
    "    \n",
    "    returns: a series indexed by ozoneID that gives back information that can then be indexed into using key's 'Latitude' and\n",
    "    'Longitude'\n",
    "    \n",
    "    NOTE: WE CAN GET ALL UNIQUE OZONE ID FROM THIS OUTPUT'S INDEX using output.index\n",
    "    \"\"\"\n",
    "    #first check that the required columns are there, otherwise print an error\n",
    "    if all(column in ozone_df.columns for column in [\"ozoneID\", \"Latitude\", \"Longitude\"]):\n",
    "        return ozone_df.groupby(\"ozoneID\").apply(lambda gr: gr[[\"Latitude\", \"Longitude\"]].iloc[0,:])\n",
    "    else:\n",
    "        raise Exception(\"one of the columns needed in ozoneID, Latitude, Longitude was missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f2c92-5acc-49bd-828f-9a8a15aec100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vincenty import vincenty_inverse\n",
    "\n",
    "def get_closest_stations(lat_long_pair, NOAA_info):\n",
    "#     print(\"working on \" + str(lat_long_pair))\n",
    "    temp = NOAA_info.copy()\n",
    "    temp[\"ozone_lat\"] = lat_long_pair[\"Latitude\"]\n",
    "    temp[\"ozone_long\"] = lat_long_pair[\"Longitude\"]\n",
    "    temp[\"vincenty_dist\"] = [vincenty_inverse((a, b), (c, d)) for a, b, c, d in zip(temp[\"lat\"], \n",
    "                                                                            temp[\"long\"], \n",
    "                                                                            temp[\"ozone_lat\"], \n",
    "                                                                            temp[\"ozone_long\"])]\n",
    "    sorted_distances = temp.sort_values(\"vincenty_dist\")[[\"StationId\",\"vincenty_dist\"]].iloc[0:10, :]\n",
    "    return sorted_distances.values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "def get_closest_stations_euclidian(lat_long_pair, NOAA_info):\n",
    "#     print(\"working on \" + str(lat_long_pair))\n",
    "    temp = NOAA_info.copy()\n",
    "    temp[\"ozone_lat\"] = lat_long_pair[\"Latitude\"]\n",
    "    temp[\"ozone_long\"] = lat_long_pair[\"Longitude\"]\n",
    "    temp[\"euclidian_dist\"] = np.sqrt((np.array(temp[\"lat\"]) - np.array(temp[\"ozone_lat\"]))**2 + (np.array(temp[\"long\"]) - np.array(temp[\"ozone_long\"]))**2)\n",
    "    \n",
    "    sorted_distances = temp.sort_values(\"euclidian_dist\")[\"StationId\"].iloc[0:10]\n",
    "    return sorted_distances.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e530e5-8a32-4fad-841f-2fa0815e2a56",
   "metadata": {},
   "source": [
    "# Clean and Join NOAA data to filtered EPA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbadfcb5-a7cf-40fe-bbbc-0ccf30118f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished code\n",
    "\n",
    "joined_data_dir = current_dir + r'/Joined Data/'\n",
    "\n",
    "headers = [\"StationId\", \"Date\", \"Measurement\", \"Value\", \"Flag1\", \"Flag2\", \"Flag3\", \"Flag4\"]\n",
    "closest_station_col_name = dict(zip(range(10), [\"station_\" + str(i) for i in range(10)]))\n",
    "\n",
    "for i in range(len(filtered_EPA_files)):\n",
    "    EPA_file = filtered_EPA_files[i]\n",
    "#     print(EPA_file)\n",
    "    NOAA_file = NOAA_files[i]\n",
    "#     print(NOAA_file)\n",
    "    year = NOAA_file[0:4]\n",
    "    \n",
    "    if year + \"EPA_NOAA_joined.csv\" in os.listdir(joined_data_dir):\n",
    "        print(\"Skipping \" + year + \", already filtered data.\")\n",
    "        continue\n",
    "    else:\n",
    "        print(\"Working on \" + year )\n",
    "        \n",
    "        print(\"Reading csvs\")\n",
    "        EPA_data = pd.read_csv(filtered_EPA_dir + EPA_file, low_memory = False, index_col = 0).drop(columns = [\"Unnamed: 0.1\"])\n",
    "        NOAA_data = pd.read_csv(NOAA_dir + \"\\\\\" + NOAA_file, names = headers)\n",
    "\n",
    "        \n",
    "        print(\"Cleaning NOAA\")\n",
    "\n",
    "        # clean NOAA_data\n",
    "        NOAA_data = NOAA_data[NOAA_data[\"StationId\"].str.startswith(\"US\")] # selecting only US data\n",
    "\n",
    " \n",
    "        NOAA_data = NOAA_data[NOAA_data[\"Measurement\"].str.contains(\"TMAX|TMIN|PRCP|SNOW\")].drop(columns = [\"Flag1\", \"Flag2\", \"Flag3\", \"Flag4\"])# selecting only important values and dropping uncessesary rows\n",
    "\n",
    "        NOAA_data[\"Datetime\"] = pd.to_datetime(NOAA_data[\"Date\"], format='%Y%m%d', errors='ignore') # adding datetime to NOAA_data \n",
    "\n",
    "        unique_station_ids = pd.DataFrame({\"StationId\":NOAA_data[\"StationId\"].unique()})\n",
    "        unique_station_ids = unique_station_ids.merge(us_station_codes, on = \"StationId\", how = \"left\").drop(columns = [\"elev\", \"name\"])\n",
    "\n",
    "        \n",
    "        # get closest stations of the available stations \n",
    "        closest_stations = {}\n",
    "        ozone_ID_coords = get_ozoneID_coords(EPA_data)\n",
    "        unique_ozone_ids = ozone_ID_coords.index\n",
    "\n",
    "        print(\"Getting closest stations\")\n",
    "        for i in tqdm(range(len(unique_ozone_ids))):\n",
    "            # check station ids for each of the entries\n",
    "            ozone_id = unique_ozone_ids[i]\n",
    "            ozone_station_coord = ozone_ID_coords.loc[ozone_id]\n",
    "            #euclidian distance much faster but can use vincenty - just not vectorized \n",
    "            closest_stations[ozone_id] = get_closest_stations_euclidian(ozone_station_coord, unique_station_ids)\n",
    "\n",
    "        closest_station_df = pd.DataFrame(closest_stations).T.rename(columns = closest_station_col_name)\n",
    "\n",
    "        # start finding closest stations and figuring out the weathers\n",
    "\n",
    "        EPA_data[\"Datetime\"] = pd.to_datetime(EPA_data[\"Date Local\"], format='%Y-%m-%d', errors='ignore')\n",
    "        merged_EPA_data = EPA_data.merge(closest_station_df, left_on = \"ozoneID\", right_index = True)\n",
    "        merged_EPA_data\n",
    "    \n",
    "        print(\"Getting weather values\")\n",
    "        for var in [\"TMAX\", \"TMIN\", \"SNOW\", \"PRCP\"]:\n",
    "            #select smaller subset to use \n",
    "            need_weather = merged_EPA_data[[\"Datetime\"] + [\"station_\" + str(i) for i in range(10)]]\n",
    "\n",
    "            # work with one NOAA variable at a time\n",
    "            NOAA_variable_data = NOAA_data[NOAA_data[\"Measurement\"] == var].rename(columns = {\"Value\": var}).drop(columns = [\"Measurement\", \"Date\"])\n",
    "\n",
    "            for i in tqdm(range(10)): \n",
    "                current_station = \"station_\" + str(i) \n",
    "                current_variable = var + \"_\" + str(i)\n",
    "                temp = need_weather.merge(NOAA_variable_data, left_on = [\"Datetime\", current_station], right_on = [\"Datetime\", \"StationId\"],  how = \"left\")\\\n",
    "                    .drop(columns = [\"StationId\"])\\\n",
    "                    .rename(columns = {var:current_variable})\n",
    "                need_weather = temp\n",
    "\n",
    "            NOAA_values = need_weather.iloc[:, -10:].values.tolist()\n",
    "            no_na_NOAA_vals = [[value for value  in row if value  == value ] for row in NOAA_values]\n",
    "            EPA_data[var] = no_na_NOAA_vals\n",
    "\n",
    "        print(\"exporting\")\n",
    "        EPA_data.to_csv(joined_data_dir + year + \"EPA_NOAA_joined.csv\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db07b0-eb64-4e07-a7f5-916113c85eba",
   "metadata": {},
   "source": [
    "# Adding on Indicators for Implementation of Regulation\n",
    "\n",
    "Using data provided in the author's online appendix, we create indicators for which regulations were applied where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49659768-8192-4352-b441-605805e60ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "NOAA_dir = current_dir + r'\\NOAA Weather Data'\n",
    "EPA_dir = current_dir + r'\\EPA Ozone Data'\n",
    "\n",
    "joined_data_dir = current_dir + r'/Joined Data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fbde82-015a-493a-a581-3ecaafdafcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_csv_files = [file for file in os.listdir(joined_data_dir) if file.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05532fa4-4dc5-46a4-8402-b5078e23196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_indicators = pd.read_csv(\"treatment_indicators.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc60b28-f5fb-428b-83a3-0511b00d91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_value(x):\n",
    "    temp = ast.literal_eval(x)\n",
    "    if len(temp) > 0:\n",
    "        return temp[0]\n",
    "    else:\n",
    "        return temp\n",
    "\n",
    "for file in joined_csv_files:\n",
    "    print(\"Working on \" + file)\n",
    "    if \"final_\" + file in os.listdir(current_dir + \"/Final Data\"):\n",
    "        print(file + \" already joined, skipping\")\n",
    "        continue\n",
    "    else:\n",
    "        temp_df = pd.read_csv(joined_data_dir + file, index_col = 0)\n",
    "        # this step takes the longest - gets the first value from each of the list of weather variables\n",
    "        temp_df[[\"TMAX\", \"TMIN\", \"SNOW\", \"PRCP\"]] = temp_df[[\"TMAX\", \"TMIN\", \"SNOW\", \"PRCP\"]].applymap(get_first_value)\n",
    "        temp_df[\"Datetime\"] = pd.to_datetime(temp_df[\"Datetime\"])\n",
    "        temp_df[\"year\"] = pd.DatetimeIndex(temp_df[\"Datetime\"]).year\n",
    "        temp_df[\"month\"] = pd.DatetimeIndex(temp_df[\"Datetime\"]).month\n",
    "        merged_df = temp_df.merge(treatment_indicators, on = [\"fips\", \"month\", \"year\"], how = \"left\")\n",
    "        merged_df.to_csv(current_dir + \"/Final Data/\" + \"final_\" + file)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f93031-2194-4dc3-a253-3e52e02e6930",
   "metadata": {},
   "source": [
    "# Finishing Data Cleaning - Merging All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca862cd-0d53-48e0-a721-2328c1e9812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_dir = current_dir + r'/Final Data/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c7da27-84e1-43fa-874b-7fc044583632",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_csv_files = [file for file in os.listdir(final_data_dir) if file.endswith(\".csv\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ece900-2855-41d7-a155-9533cae02e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_together = []\n",
    "for file in final_csv_files:\n",
    "    join_together.append(pd.read_csv(final_data_dir + file, index_col = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c6122-38ea-48ff-9ab7-236ca95d7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final = pd.concat(join_together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bb585-8046-4818-ae52-2aeb337edf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef616fc1-ebcf-4144-ad06-401102b682ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_code_join = [1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, \n",
    "                   28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49,\n",
    "                   50, 51, 53, 54, 55, 56]\n",
    "\n",
    "census_region_join = [3, 4, 4, 3, 4, 4, 1, 3, 3, 3, 3, 4, 4, 2, 2, 2, 2, 3, 3, 1, 3, 1, 2, 2, 3, 2, 4, 2, 4, 1, 1, 4,\n",
    "              1, 3, 2, 2, 3, 4, 1, 1, 3, 2, 3, 3, 4, 1, 3, 4, 3, 2, 4]\n",
    "\n",
    "state_census_map = dict(zip(state_code_join, census_region_join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c02353-4df7-4e64-9787-4e6ea205b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add census region\n",
    "\n",
    "final[\"census_region\"] = final[\"State Code\"].map(state_census_map)\n",
    "\n",
    "# remove mexico\n",
    "\n",
    "indicator = [state_code in state_code_join for state_code in final[\"State Code\"]]\n",
    "\n",
    "final_data = final[indicator]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bcb782-4203-4c6e-9ef7-c8afe8d72063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data.to_csv(\"final_cleaned_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969725f-038e-42c6-b6d2-85320caaac42",
   "metadata": {},
   "source": [
    "# Reproducing Author's Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da35c1d7-8804-42dc-8ec8-79791bc94a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danny\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (16,17,18,19,22,23,24,25) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\Danny\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "final_data = pd.read_csv('final_cleaned_data.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a318dab-1ebd-4afe-b45f-81665049f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created lagged temperature\n",
    "final_data[\"Datetime\"] = pd.to_datetime(final_data[\"Datetime\"])\n",
    "final_data[\"day_before\"] = [date - timedelta(days=1) for date in final_data[\"Datetime\"]]\n",
    "lagged_weather = final_data[[\"TMAX\", \"TMIN\", \"SNOW\", \"PRCP\", \"ozoneID\", \"Datetime\"]].rename(columns = {\"TMAX\":\"lagged_TMAX\", \"TMIN\":\"lagged_TMIN\", \"SNOW\":\"lagged_SNOW\", \"PRCP\":\"lagged_PRCP\", \"Datetime\":\"day_before\"})\n",
    "merged_lagged_weather = final_data.merge(lagged_weather, on = [\"ozoneID\", \"day_before\"]).rename(columns = {\"Daily Max Ozone\":\"mean_ozone\", \"Daily 8hr Mean Ozone\":\"8hr_mean_ozone\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3563df66-1164-4f7f-b13d-4ccf802b68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summer_months = [6, 7, 8]\n",
    "summer_only_indicator = [month in summer_months for month in merged_lagged_weather[\"month\"]]\n",
    "summer_only = merged_lagged_weather.copy()[summer_only_indicator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ad8956-7ef3-4d5e-9436-4ef0e59091f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danny\\Anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py:358: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "summer_only[\"ln_ozone\"] = np.log(summer_only[\"mean_ozone\"])\n",
    "summer_only[\"ln_8hr_ozone\"] = np.log(summer_only[\"8hr_mean_ozone\"])\n",
    "summer_only = summer_only.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d82b00f5-2d42-4069-90ed-01dc80c812e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State Code</th>\n",
       "      <th>County Code</th>\n",
       "      <th>Site Num</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Date Local</th>\n",
       "      <th>Sample Measurement</th>\n",
       "      <th>State Name</th>\n",
       "      <th>County Name</th>\n",
       "      <th>ozoneID</th>\n",
       "      <th>mean_ozone</th>\n",
       "      <th>8hr_mean_ozone</th>\n",
       "      <th>fips</th>\n",
       "      <th>urban</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>RVPStart</th>\n",
       "      <th>RVPEnd</th>\n",
       "      <th>RFGStart</th>\n",
       "      <th>RFGEnd</th>\n",
       "      <th>RegFlag</th>\n",
       "      <th>RVPI</th>\n",
       "      <th>treat_rvpII</th>\n",
       "      <th>treat_rfg</th>\n",
       "      <th>treat_rvpI</th>\n",
       "      <th>treat_CARB</th>\n",
       "      <th>TreatRFG</th>\n",
       "      <th>panelid</th>\n",
       "      <th>RFGStart2</th>\n",
       "      <th>RFGEnd2</th>\n",
       "      <th>TreatRVPII</th>\n",
       "      <th>RVPStart2</th>\n",
       "      <th>RVPEnd2</th>\n",
       "      <th>TreatCARB</th>\n",
       "      <th>TreatRFGCA</th>\n",
       "      <th>RVPCty</th>\n",
       "      <th>RFGCty</th>\n",
       "      <th>CARBCty</th>\n",
       "      <th>TreatRVPca</th>\n",
       "      <th>census_region</th>\n",
       "      <th>day_before</th>\n",
       "      <th>lagged_TMAX</th>\n",
       "      <th>lagged_TMIN</th>\n",
       "      <th>lagged_SNOW</th>\n",
       "      <th>lagged_PRCP</th>\n",
       "      <th>ln_ozone</th>\n",
       "      <th>ln_8hr_ozone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>1003</td>\n",
       "      <td>33.485556</td>\n",
       "      <td>-86.915</td>\n",
       "      <td>1989-06-01</td>\n",
       "      <td>0.002</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>(1, 73, 1003)</td>\n",
       "      <td>0.025667</td>\n",
       "      <td>0.049375</td>\n",
       "      <td>1073</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1989-06-01</td>\n",
       "      <td>328.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1989</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1989-05-31</td>\n",
       "      <td>328.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.662562</td>\n",
       "      <td>-3.008311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>1003</td>\n",
       "      <td>33.485556</td>\n",
       "      <td>-86.915</td>\n",
       "      <td>1989-06-02</td>\n",
       "      <td>0.002</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>(1, 73, 1003)</td>\n",
       "      <td>0.012250</td>\n",
       "      <td>0.025875</td>\n",
       "      <td>1073</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1989-06-02</td>\n",
       "      <td>311.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1989</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1989-06-01</td>\n",
       "      <td>328.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.402229</td>\n",
       "      <td>-3.654478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>1003</td>\n",
       "      <td>33.485556</td>\n",
       "      <td>-86.915</td>\n",
       "      <td>1989-06-03</td>\n",
       "      <td>0.002</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>(1, 73, 1003)</td>\n",
       "      <td>0.016875</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>1073</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1989-06-03</td>\n",
       "      <td>294.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1989</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1989-06-02</td>\n",
       "      <td>311.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>-4.081922</td>\n",
       "      <td>-3.523365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>1003</td>\n",
       "      <td>33.485556</td>\n",
       "      <td>-86.915</td>\n",
       "      <td>1989-06-04</td>\n",
       "      <td>0.006</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>(1, 73, 1003)</td>\n",
       "      <td>0.016250</td>\n",
       "      <td>0.027875</td>\n",
       "      <td>1073</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1989-06-04</td>\n",
       "      <td>272.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>1989</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1989-06-03</td>\n",
       "      <td>294.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-4.119662</td>\n",
       "      <td>-3.580025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>1003</td>\n",
       "      <td>33.485556</td>\n",
       "      <td>-86.915</td>\n",
       "      <td>1989-06-05</td>\n",
       "      <td>0.012</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Jefferson</td>\n",
       "      <td>(1, 73, 1003)</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>0.018250</td>\n",
       "      <td>1073</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1989-06-05</td>\n",
       "      <td>272.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>1989</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1989-06-04</td>\n",
       "      <td>272.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>-4.177726</td>\n",
       "      <td>-4.003590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    State Code  County Code  Site Num   Latitude  Longitude  Date Local  \\\n",
       "91           1           73      1003  33.485556    -86.915  1989-06-01   \n",
       "92           1           73      1003  33.485556    -86.915  1989-06-02   \n",
       "93           1           73      1003  33.485556    -86.915  1989-06-03   \n",
       "94           1           73      1003  33.485556    -86.915  1989-06-04   \n",
       "95           1           73      1003  33.485556    -86.915  1989-06-05   \n",
       "\n",
       "    Sample Measurement State Name County Name        ozoneID  mean_ozone  \\\n",
       "91               0.002    Alabama   Jefferson  (1, 73, 1003)    0.025667   \n",
       "92               0.002    Alabama   Jefferson  (1, 73, 1003)    0.012250   \n",
       "93               0.002    Alabama   Jefferson  (1, 73, 1003)    0.016875   \n",
       "94               0.006    Alabama   Jefferson  (1, 73, 1003)    0.016250   \n",
       "95               0.012    Alabama   Jefferson  (1, 73, 1003)    0.015333   \n",
       "\n",
       "    8hr_mean_ozone  fips  urban   Datetime   TMAX   TMIN SNOW   PRCP  year  \\\n",
       "91        0.049375  1073    2.0 1989-06-01  328.0  217.0  0.0    0.0  1989   \n",
       "92        0.025875  1073    2.0 1989-06-02  311.0  200.0  0.0   79.0  1989   \n",
       "93        0.029500  1073    2.0 1989-06-03  294.0  194.0  0.0   33.0  1989   \n",
       "94        0.027875  1073    2.0 1989-06-04  272.0  194.0  0.0  292.0  1989   \n",
       "95        0.018250  1073    2.0 1989-06-05  272.0  194.0  0.0  221.0  1989   \n",
       "\n",
       "    month RVPStart RVPEnd RFGStart RFGEnd  RegFlag  RVPI  treat_rvpII  \\\n",
       "91      6      NaN    NaN      NaN    NaN      0.0  10.5          0.0   \n",
       "92      6      NaN    NaN      NaN    NaN      0.0  10.5          0.0   \n",
       "93      6      NaN    NaN      NaN    NaN      0.0  10.5          0.0   \n",
       "94      6      NaN    NaN      NaN    NaN      0.0  10.5          0.0   \n",
       "95      6      NaN    NaN      NaN    NaN      0.0  10.5          0.0   \n",
       "\n",
       "    treat_rfg  treat_rvpI  treat_CARB  TreatRFG  panelid  RFGStart2  RFGEnd2  \\\n",
       "91        0.0         1.0         0.0       0.0     12.0        NaN      NaN   \n",
       "92        0.0         1.0         0.0       0.0     12.0        NaN      NaN   \n",
       "93        0.0         1.0         0.0       0.0     12.0        NaN      NaN   \n",
       "94        0.0         1.0         0.0       0.0     12.0        NaN      NaN   \n",
       "95        0.0         1.0         0.0       0.0     12.0        NaN      NaN   \n",
       "\n",
       "    TreatRVPII  RVPStart2  RVPEnd2  TreatCARB  TreatRFGCA  RVPCty  RFGCty  \\\n",
       "91         0.0        NaN      NaN        0.0         0.0     1.0     0.0   \n",
       "92         0.0        NaN      NaN        0.0         0.0     1.0     0.0   \n",
       "93         0.0        NaN      NaN        0.0         0.0     1.0     0.0   \n",
       "94         0.0        NaN      NaN        0.0         0.0     1.0     0.0   \n",
       "95         0.0        NaN      NaN        0.0         0.0     1.0     0.0   \n",
       "\n",
       "    CARBCty  TreatRVPca  census_region day_before lagged_TMAX lagged_TMIN  \\\n",
       "91      0.0         0.0            3.0 1989-05-31       328.0       217.0   \n",
       "92      0.0         0.0            3.0 1989-06-01       328.0       217.0   \n",
       "93      0.0         0.0            3.0 1989-06-02       311.0       200.0   \n",
       "94      0.0         0.0            3.0 1989-06-03       294.0       194.0   \n",
       "95      0.0         0.0            3.0 1989-06-04       272.0       194.0   \n",
       "\n",
       "   lagged_SNOW lagged_PRCP  ln_ozone  ln_8hr_ozone  \n",
       "91         0.0         0.0 -3.662562     -3.008311  \n",
       "92         0.0         0.0 -4.402229     -3.654478  \n",
       "93         0.0        79.0 -4.081922     -3.523365  \n",
       "94         0.0        33.0 -4.119662     -3.580025  \n",
       "95         0.0       292.0 -4.177726     -4.003590  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summer_only.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaabf37b-78ea-4cd2-89da-6842e8d5d66e",
   "metadata": {},
   "source": [
    "# Full Regression 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1607ca1-463a-4d4e-a94c-1fca0135e4af",
   "metadata": {},
   "source": [
    "# Simplifying Assumption : Only use states as fixed effects, not individual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6caf0fe7-7114-4eab-bfd1-c68d4b77ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD regression 1\n",
    "dependent_vars = [\"ln_ozone\", \"ln_8hr_ozone\"]\n",
    "treatment_vars = [\"treat_rvpI\", \"treat_rvpII\", \"treat_rfg\", \"treat_CARB\"]\n",
    "independent_vars = [\"year\", \"census_region\", \"State Code\"]\n",
    "control = summer_only.copy()[independent_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cfe361f8-2c03-4e44-949e-bcafdb6605e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating census interacted with year for the first DD regression\n",
    "control[\"year\"] = control[\"year\"].astype(str) \n",
    "control[\"census_region\"] = control[\"census_region\"].astype(int).astype(str) \n",
    "control[\"State Code\"] = control[\"State Code\"].astype(str) \n",
    "control[\"census_region_year_interaction\"] = control[\"census_region\"].str.cat(control[\"year\"], sep=' + ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9cd74af2-57ba-46fb-a170-8c640af9cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = pd.get_dummies(control, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5408c3de-92b3-482a-924e-39855ab5f2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on dependent, ln_ozone\n",
      "working on treatment, treat_rvpI\n",
      "working on treatment, treat_rvpII\n",
      "working on treatment, treat_rfg\n",
      "working on treatment, treat_CARB\n",
      "working on dependent, ln_8hr_ozone\n",
      "working on treatment, treat_rvpI\n",
      "working on treatment, treat_rvpII\n",
      "working on treatment, treat_rfg\n",
      "working on treatment, treat_CARB\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for i in range(len(dependent_vars)):\n",
    "    \n",
    "    dependent_var = dependent_vars[i]\n",
    "    print(\"working on dependent, \" + dependent_var)\n",
    "    results[dependent_var] = {}\n",
    "    \n",
    "    for j in range(len(treatment_vars)):\n",
    "        \n",
    "        treatment_var = treatment_vars[j]\n",
    "        print(\"working on treatment, \" + treatment_var)\n",
    "        \n",
    "        df = pd.concat([summer_only[dependent_var], summer_only[treatment_var], control], axis = 1)\n",
    "        df = df.dropna(axis = 0, how = \"any\")\n",
    "\n",
    "        y = df[dependent_var]\n",
    "        X = df.drop(columns = [dependent_var])\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        \n",
    "        values = {\"Point Estimate\":model.params[treatment_var], \n",
    "                  \"Standard Error\":model.bse[treatment_var],\n",
    "                  \"P-Value\":model.pvalues[treatment_var],\n",
    "                  \"R Squared\":model.rsquared}\n",
    "        results[dependent_var][treatment_var] = values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a0af7b76-821d-4a32-94f0-4b12dba8eda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Point Estimate</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>R Squared</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_ozone</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>treat_rvpI</th>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.7654</td>\n",
       "      <td>0.1318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat_rvpII</th>\n",
       "      <td>-0.0197</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat_rfg</th>\n",
       "      <td>-0.0521</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat_CARB</th>\n",
       "      <td>-0.0778</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Point Estimate  Standard Error  P-Value  R Squared\n",
       "ln_ozone                                                       \n",
       "treat_rvpI          -0.0011          0.0037   0.7654     0.1318\n",
       "treat_rvpII         -0.0197          0.0014   0.0000     0.1321\n",
       "treat_rfg           -0.0521          0.0015   0.0000     0.1331\n",
       "treat_CARB          -0.0778          0.0031   0.0000     0.1325"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_1 = pd.DataFrame(results[\"ln_ozone\"])\n",
    "result_df_1 = result_df_1.applymap(lambda x: round(x, 4))\n",
    "result_df_1 = result_df_1.T\n",
    "index1 = result_df_1.index\n",
    "index1.name = \"ln_ozone\"\n",
    "result_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7f896aae-17c4-495b-b053-c33baeb0916b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Point Estimate</th>\n",
       "      <th>Standard Error</th>\n",
       "      <th>P-Value</th>\n",
       "      <th>R Squared</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_8hr_ozone</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>treat_rvpI</th>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.3938</td>\n",
       "      <td>0.1167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat_rvpII</th>\n",
       "      <td>-0.0031</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.1170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat_rfg</th>\n",
       "      <td>-0.0302</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat_CARB</th>\n",
       "      <td>-0.0868</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Point Estimate  Standard Error  P-Value  R Squared\n",
       "ln_8hr_ozone                                                    \n",
       "treat_rvpI            0.0031          0.0036   0.3938     0.1167\n",
       "treat_rvpII          -0.0031          0.0013   0.0136     0.1170\n",
       "treat_rfg            -0.0302          0.0014   0.0000     0.1173\n",
       "treat_CARB           -0.0868          0.0029   0.0000     0.1175"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_2 = pd.DataFrame(results[\"ln_8hr_ozone\"])\n",
    "result_df_2 = result_df_2.applymap(lambda x: round(x, 4))\n",
    "result_df_2 = result_df_2.T\n",
    "index2 = result_df_2.index\n",
    "index2.name = \"ln_8hr_ozone\"\n",
    "result_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b91bde-ab94-46c4-b891-d71028354299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
